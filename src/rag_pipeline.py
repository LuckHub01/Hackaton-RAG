
"""
PIPELINE RAG COMPLET - Culture Burkinab√®
Version finale avec HuggingFace Router API
"""

import json
import time
from typing import List, Dict
import os

# Open Source Libraries
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
load_dotenv()


class CultureRAGPipeline:
    """Pipeline RAG pour questions-r√©ponses sur la culture burkinab√®"""
    
    def __init__(
        self,
        corpus_file: str,
        model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        vector_db_path: str = "data/vectors/chroma_db",
        top_k: int = 5
    ):
        """
        Initialisation du pipeline RAG
        """
        self.corpus_file = corpus_file
        self.top_k = top_k
        
        print("üöÄ Initialisation du pipeline RAG...")
        
        # 1. Chargement du corpus
        print("üìö Chargement du corpus...")
        with open(corpus_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.corpus = data['corpus']
        
        print(f"‚úÖ {len(self.corpus)} chunks charg√©s")
        
        # 2. Mod√®le d'embeddings
        print(f"üß† Chargement du mod√®le d'embeddings: {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        print(f"‚úÖ Dimension des vecteurs: {self.embedding_model.get_sentence_embedding_dimension()}")
        
        # 3. Base vectorielle ChromaDB
        print("üíæ Initialisation de ChromaDB...")
        self.chroma_client = chromadb.PersistentClient(
            path=vector_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Cr√©er ou r√©cup√©rer la collection
        self.collection_name = "culture_burkina"
        try:
            self.collection = self.chroma_client.get_collection(self.collection_name)
            print(f"‚úÖ Collection '{self.collection_name}' charg√©e ({self.collection.count()} documents)")
        except:
            self.collection = None
            print("‚ö†Ô∏è Collection non trouv√©e, il faut l'indexer")
    
    def index_corpus(self):
        """Indexation du corpus dans ChromaDB"""
        print("\nüîÑ INDEXATION DU CORPUS")
        print("="*50)
        
        try:
            self.chroma_client.delete_collection(self.collection_name)
            print("üóëÔ∏è Ancienne collection supprim√©e")
        except:
            pass
        
        self.collection = self.chroma_client.create_collection(
            name=self.collection_name,
            metadata={"description": "Articles culture burkinab√® - LeFaso.net"}
        )
        
        texts = []
        metadatas = []
        ids = []
        
        for doc in self.corpus:
            full_text = f"Titre: {doc['title']}\n\nContenu: {doc['content']}"
            texts.append(full_text)
            
            metadatas.append({
                'article_id': doc['article_id'],
                'url': doc['url'],
                'title': doc['title'],
                'date': doc['date'],
                'category': doc['category'],
                'chunk_index': str(doc['chunk_index']),
                'content': doc['content']
            })
            
            ids.append(doc['id'])
        
        print(f"üî¢ G√©n√©ration des embeddings pour {len(texts)} documents...")
        batch_size = 32
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            embeddings = self.embedding_model.encode(
                batch,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            all_embeddings.extend(embeddings.tolist())
            print(f"  Batch {i//batch_size + 1}/{(len(texts)-1)//batch_size + 1} trait√©")
        
        print("üíæ Ajout des documents √† ChromaDB...")
        self.collection.add(
            embeddings=all_embeddings,
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"‚úÖ Indexation termin√©e: {self.collection.count()} documents")
        print("="*50)
    
    def retrieve(self, query: str, top_k: int = None) -> List[Dict]:
        """R√©cup√©ration des documents pertinents"""
        if top_k is None:
            top_k = self.top_k
        
        query_embedding = self.embedding_model.encode(query).tolist()
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=['documents', 'metadatas', 'distances']
        )
        
        retrieved_docs = []
        for i in range(len(results['ids'][0])):
            doc = {
                'id': results['ids'][0][i],
                'content': results['metadatas'][0][i]['content'],
                'title': results['metadatas'][0][i]['title'],
                'url': results['metadatas'][0][i]['url'],
                'date': results['metadatas'][0][i]['date'],
                'distance': results['distances'][0][i],
                'similarity_score': 1 - results['distances'][0][i]
            }
            retrieved_docs.append(doc)
        
        return retrieved_docs
    
    def generate_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:
        """G√©n√©ration du prompt pour le LLM"""
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            context_parts.append(
                f"[Document {i}]\n"
                f"Titre: {doc['title']}\n"
                f"Date: {doc['date']}\n"
                f"Contenu: {doc['content'][:500]}...\n"  # Limiter la longueur
                f"Source: {doc['url']}\n"
            )
        
        context = "\n---\n".join(context_parts)
        
        prompt = f"""Tu es un assistant expert sur la culture burkinab√®. R√©ponds √† la question en te basant UNIQUEMENT sur les documents fournis ci-dessous.

DOCUMENTS DE R√âF√âRENCE:
{context}

---

QUESTION: {query}

INSTRUCTIONS:
1. R√©ponds en fran√ßais de mani√®re claire et pr√©cise
2. Utilise UNIQUEMENT les informations des documents ci-dessus
3. Si l'information n'est pas dans les documents, dis "Je n'ai pas trouv√© cette information dans ma base de donn√©es"
4. Cite les sources en mentionnant les titres des articles
5. Structure ta r√©ponse avec des paragraphes si n√©cessaire

R√âPONSE:"""
        
        return prompt
    
    def generate_answer_huggingface(self, prompt: str) -> str:
        """
        G√©n√©ration avec HuggingFace Router API
        Utilise Mistral-7B via le router HuggingFace
        """
        try:
            import requests
            
            # R√©cup√©rer le token (supporte HF_TOKEN ou HUGGINGFACE_TOKEN)
            hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN')
            
            if not hf_token:
                return "‚ùå Token HuggingFace manquant. Ajouter HF_TOKEN ou HUGGINGFACE_TOKEN dans .env"
            
            # URL de l'API Router HuggingFace
            API_URL = "https://router.huggingface.co/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {hf_token}",
                "Content-Type": "application/json"
            }
            
            # Payload au format OpenAI (chat completions)
            payload = {
                "model": "mistralai/Mistral-7B-Instruct-v0.2:featherless-ai",
                "messages": [
                    {
                        "role": "system",
                        "content": "Tu es un assistant expert sur la culture burkinab√®. R√©ponds en fran√ßais."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.7,
                "top_p": 0.9
            }
            
            print("   üîÑ Appel de l'API HuggingFace Router...")
            
            response = requests.post(API_URL, headers=headers, json=payload, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                
                # Extraire le message de la r√©ponse
                if "choices" in result and len(result["choices"]) > 0:
                    message = result["choices"][0]["message"]["content"]
                    print("   ‚úÖ R√©ponse g√©n√©r√©e avec succ√®s")
                    return message
                else:
                    return "‚ùå Format de r√©ponse inattendu"
                    
            elif response.status_code == 401:
                return "‚ùå Token HuggingFace invalide. V√©rifiez votre token dans .env"
            elif response.status_code == 429:
                return "‚ùå Limite de requ√™tes atteinte. R√©essayez dans quelques minutes."
            elif response.status_code == 503:
                return "‚è≥ Mod√®le en cours de chargement. R√©essayez dans 30 secondes."
            else:
                return f"‚ùå Erreur API: {response.status_code} - {response.text[:200]}"
                
        except requests.exceptions.Timeout:
            return "‚è±Ô∏è Timeout: La requ√™te a pris trop de temps. R√©essayez."
        except Exception as e:
            return f"‚ùå Erreur: {str(e)}"
    
    def generate_answer_local(self, prompt: str) -> str:
        """
        G√©n√©ration de r√©ponse avec LLM local (Ollama)
        Optionnel - n√©cessite Ollama install√©
        """
        try:
            import requests
            
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': 'mistral',
                    'prompt': prompt,
                    'stream': False,
                    'options': {
                        'temperature': 0.7,
                        'top_p': 0.9,
                        'num_predict': 500
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()['response']
            else:
                return "‚ùå Ollama non disponible. Installez Ollama ou utilisez HuggingFace."
        
        except Exception as e:
            return f"‚ùå Erreur Ollama: {str(e)}\nInstallez Ollama: https://ollama.ai/"
    
    def answer_question(
        self,
        query: str,
        use_local_llm: bool = False,
        verbose: bool = True
    ) -> Dict:
        """
        Pipeline complet: Question ‚Üí R√©ponse
        
        Args:
            query: Question de l'utilisateur
            use_local_llm: True=Ollama local, False=HuggingFace API
            verbose: Afficher les √©tapes
        """
        start_time = time.time()
        
        if verbose:
            print(f"\n‚ùì Question: {query}")
            print("="*50)
        
        # √âtape 1: Retrieval
        if verbose:
            print("üîç Recherche de documents pertinents...")
        
        retrieved_docs = self.retrieve(query)
        
        if verbose:
            print(f"‚úÖ {len(retrieved_docs)} documents trouv√©s")
            for i, doc in enumerate(retrieved_docs, 1):
                print(f"  {i}. {doc['title'][:60]}... (score: {doc['similarity_score']:.3f})")
        
        # √âtape 2: G√©n√©ration du prompt
        prompt = self.generate_prompt(query, retrieved_docs)
        
        # √âtape 3: G√©n√©ration de la r√©ponse
        if verbose:
            print("\nü§ñ G√©n√©ration de la r√©ponse...")
        
        if use_local_llm:
            answer = self.generate_answer_local(prompt)
        else:
            answer = self.generate_answer_huggingface(prompt)
        
        # Calcul du temps
        elapsed_time = time.time() - start_time
        
        if verbose:
            print(f"\n‚è±Ô∏è Temps de r√©ponse: {elapsed_time:.2f}s")
            print("="*50)
        
        # R√©sultat complet
        result = {
            'question': query,
            'answer': answer,
            'sources': [
                {
                    'title': doc['title'],
                    'url': doc['url'],
                    'date': doc['date'],
                    'content': doc['content'],
                    'relevance_score': doc['similarity_score']
                }
                for doc in retrieved_docs
            ],
            'response_time': elapsed_time,
            'num_docs_retrieved': len(retrieved_docs)
        }
        
        return result


def test_rag():
    """Test rapide du pipeline"""
    
    print("\n" + "üáßüá´"*30)
    print("\nTEST DU PIPELINE RAG - Culture Burkinab√®")
    print("\n" + "üáßüá´"*30 + "\n")
    
    # Initialisation
    rag = CultureRAGPipeline(
        corpus_file="data/processed/corpus_cleaned.json",
        top_k=5
    )
    
    # Indexer si n√©cessaire
    if rag.collection is None or rag.collection.count() == 0:
        print("‚ö†Ô∏è Collection non index√©e. Indexation en cours...")
        rag.index_corpus()
    
    # Questions test
    questions = [
        "Quels sont les principaux festivals culturels au Burkina Faso?",
        "Parle-moi de la musique burkinab√®",
        "Qui est Alif Naaba?",
        "Qu'est-ce que le BBDA?"
    ]
    
    print("\n" + "="*60)
    print("TESTS DES QUESTIONS")
    print("="*60)
    
    for i, q in enumerate(questions, 1):
        print(f"\n\n{'='*60}")
        print(f"QUESTION {i}/{len(questions)}")
        print('='*60)
        
        result = rag.answer_question(q, use_local_llm=False)
        
        print(f"\nüí¨ R√âPONSE:")
        print("-"*60)
        print(result['answer'])
        print("-"*60)
        
        print(f"\nüìö Sources ({len(result['sources'])}):")
        for j, source in enumerate(result['sources'], 1):
            print(f"  {j}. {source['title']}")
            print(f"     Pertinence: {source['relevance_score']:.2f}")
            print(f"     {source['url']}")
        
        print(f"\n‚è±Ô∏è Temps: {result['response_time']:.2f}s")
        
        time.sleep(2)  # Pause entre les questions
    
    print("\n\n" + "="*60)
    print("‚úÖ TEST TERMIN√â")
    print("="*60)


if __name__ == "__main__":
    test_rag()